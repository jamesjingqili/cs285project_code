{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  60%|######    | 1440/2400 [00:16<00:10, 88.78it/s, env_step=16000, len=200.00, n/ep=8.00, n/st=1600.00, rew=-185.96, rew_std=130.38, v/ep=25.28, v/st=5056.09]                                             \n",
      "Epoch #1:   0%|          | 0/2400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_result': '-222.02 ± 120.40',\n",
      " 'best_reward': -222.02202675443905,\n",
      " 'duration': '16.22s',\n",
      " 'test_episode': 100.0,\n",
      " 'test_speed': '13745.45 step/s',\n",
      " 'test_step': 20000,\n",
      " 'test_time': '1.46s',\n",
      " 'train_episode': 80.0,\n",
      " 'train_speed': '1083.53 step/s',\n",
      " 'train_step': 16000,\n",
      " 'train_time/collector': '3.05s',\n",
      " 'train_time/model': '11.72s'}\n",
      "Final reward: -132.40889986952368, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  60%|######    | 1440/2400 [00:17<00:11, 82.62it/s, env_step=16000, len=200.00, n/ep=8.00, n/st=1600.00, rew=-177.92, rew_std=155.62, v/ep=26.32, v/st=5264.67]                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_result': '-172.05 ± 117.14',\n",
      " 'best_reward': -172.054880058112,\n",
      " 'duration': '17.43s',\n",
      " 'test_episode': 200.0,\n",
      " 'test_speed': '14034.90 step/s',\n",
      " 'test_step': 40000,\n",
      " 'test_time': '2.85s',\n",
      " 'train_episode': 80.0,\n",
      " 'train_speed': '1097.39 step/s',\n",
      " 'train_step': 16000,\n",
      " 'train_time/collector': '3.06s',\n",
      " 'train_time/model': '11.52s'}\n",
      "Final reward: -124.99528559181373, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 481it [00:03, 136.55it/s, env_step=8000, len=200, n/ep=8, n/st=1600, rew=-317.25, v/ep=25.98, v/st=5196.65]                         \n",
      "Epoch #2:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -604.957454 ± 470.947396, best_reward: -604.957454 ± 470.947396 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 481it [00:03, 140.45it/s, env_step=17600, len=200, n/ep=8, n/st=1600, rew=-281.49, v/ep=26.30, v/st=5259.27]                         \n",
      "Epoch #3:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -750.766964 ± 91.319559, best_reward: -604.957454 ± 470.947396 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 481it [00:03, 140.87it/s, env_step=27200, len=200, n/ep=8, n/st=1600, rew=-170.78, v/ep=25.97, v/st=5194.41]                         \n",
      "Epoch #4:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -698.085307 ± 113.068231, best_reward: -604.957454 ± 470.947396 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 481it [00:03, 138.05it/s, env_step=36800, len=200, n/ep=8, n/st=1600, rew=-217.66, v/ep=24.95, v/st=4989.43]                         \n",
      "Epoch #5:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -528.700706 ± 154.796381, best_reward: -528.700706 ± 154.796381 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 481it [00:03, 136.27it/s, env_step=46400, len=200, n/ep=8, n/st=1600, rew=-208.67, v/ep=23.46, v/st=4692.60]                         \n",
      "Epoch #6:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -404.197649 ± 224.046713, best_reward: -404.197649 ± 224.046713 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 481it [00:03, 138.71it/s, env_step=56000, len=200, n/ep=8, n/st=1600, rew=-214.63, v/ep=26.02, v/st=5203.12]                         \n",
      "Epoch #7:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -491.545951 ± 168.746814, best_reward: -404.197649 ± 224.046713 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 481it [00:03, 137.44it/s, env_step=65600, len=200, n/ep=8, n/st=1600, rew=-225.14, v/ep=26.59, v/st=5318.13]                         \n",
      "Epoch #8:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -507.884893 ± 191.929256, best_reward: -404.197649 ± 224.046713 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 481it [00:03, 136.65it/s, env_step=75200, len=200, n/ep=8, n/st=1600, rew=-318.80, v/ep=25.86, v/st=5172.70]                         \n",
      "Epoch #9:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -597.134398 ± 243.003785, best_reward: -404.197649 ± 224.046713 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 481it [00:03, 139.98it/s, env_step=84800, len=200, n/ep=8, n/st=1600, rew=-160.56, v/ep=26.62, v/st=5323.62]                         \n",
      "Epoch #10:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -548.594743 ± 158.957307, best_reward: -404.197649 ± 224.046713 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 481it [00:03, 139.70it/s, env_step=94400, len=200, n/ep=8, n/st=1600, rew=-260.96, v/ep=26.13, v/st=5226.04]                         \n",
      "Epoch #11:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -392.737516 ± 255.773762, best_reward: -392.737516 ± 255.773762 in #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 481it [00:03, 140.18it/s, env_step=104000, len=200, n/ep=8, n/st=1600, rew=-205.89, v/ep=26.56, v/st=5312.85]                         \n",
      "Epoch #12:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: -368.477263 ± 222.701019, best_reward: -368.477263 ± 222.701019 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 481it [00:03, 140.47it/s, env_step=113600, len=200, n/ep=8, n/st=1600, rew=-208.01, v/ep=26.70, v/st=5340.30]                         \n",
      "Epoch #13:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: -485.146436 ± 195.913573, best_reward: -368.477263 ± 222.701019 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 481it [00:03, 139.41it/s, env_step=123200, len=200, n/ep=8, n/st=1600, rew=-196.44, v/ep=26.24, v/st=5248.08]                         \n",
      "Epoch #14:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: -369.032541 ± 252.038044, best_reward: -368.477263 ± 222.701019 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 481it [00:03, 138.89it/s, env_step=132800, len=200, n/ep=8, n/st=1600, rew=-245.36, v/ep=25.00, v/st=5000.06]                         \n",
      "Epoch #15:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: -372.231960 ± 263.245327, best_reward: -368.477263 ± 222.701019 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 481it [00:03, 140.07it/s, env_step=142400, len=200, n/ep=8, n/st=1600, rew=-255.63, v/ep=26.42, v/st=5284.06]                         \n",
      "Epoch #16:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: -340.367904 ± 219.201510, best_reward: -340.367904 ± 219.201510 in #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 481it [00:03, 140.70it/s, env_step=152000, len=200, n/ep=8, n/st=1600, rew=-264.18, v/ep=26.53, v/st=5305.46]                         \n",
      "Epoch #17:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: -543.094916 ± 280.280518, best_reward: -340.367904 ± 219.201510 in #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 481it [00:03, 139.71it/s, env_step=161600, len=200, n/ep=8, n/st=1600, rew=-230.84, v/ep=26.26, v/st=5252.09]                         \n",
      "Epoch #18:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: -452.765935 ± 275.591989, best_reward: -340.367904 ± 219.201510 in #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 481it [00:03, 140.04it/s, env_step=171200, len=200, n/ep=8, n/st=1600, rew=-255.96, v/ep=26.58, v/st=5315.14]                         \n",
      "Epoch #19:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: -320.896624 ± 228.002984, best_reward: -320.896624 ± 228.002984 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 481it [00:03, 139.55it/s, env_step=180800, len=200, n/ep=8, n/st=1600, rew=-177.45, v/ep=26.05, v/st=5209.48]                         \n",
      "Epoch #20:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: -394.481045 ± 252.956734, best_reward: -320.896624 ± 228.002984 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 481it [00:03, 140.40it/s, env_step=190400, len=200, n/ep=8, n/st=1600, rew=-208.60, v/ep=26.68, v/st=5335.31]                         \n",
      "Epoch #1:   0%|          | 0/2400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: -332.793022 ± 208.452075, best_reward: -320.896624 ± 228.002984 in #18\n",
      "{'best_result': '-320.90 ± 228.00',\n",
      " 'best_reward': -320.89662382868005,\n",
      " 'duration': '97.64s',\n",
      " 'test_episode': 2000.0,\n",
      " 'test_speed': '14062.94 step/s',\n",
      " 'test_step': 400000,\n",
      " 'test_time': '28.44s',\n",
      " 'train_episode': 480.0,\n",
      " 'train_speed': '1387.37 step/s',\n",
      " 'train_step': 96000,\n",
      " 'train_time/collector': '18.46s',\n",
      " 'train_time/model': '50.73s'}\n",
      "Final reward: -253.36232680599616, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   0%|          | 0/2400 [00:01<?, ?it/s, env_step=1600, len=200.00, n/ep=8.00, n/st=1600.00, rew=-240.19, rew_std=73.05, v/ep=25.69, v/st=5137.35]\n",
      "Epoch #1:   0%|          | 0/2400 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_result': '-208.34 ± 127.22',\n",
      " 'best_reward': -208.33845814146082,\n",
      " 'duration': '1.74s',\n",
      " 'test_episode': 100.0,\n",
      " 'test_speed': '14047.58 step/s',\n",
      " 'test_step': 20000,\n",
      " 'test_time': '1.42s',\n",
      " 'train_episode': 8.0,\n",
      " 'train_speed': '5046.60 step/s',\n",
      " 'train_step': 1600,\n",
      " 'train_time/collector': '0.31s',\n",
      " 'train_time/model': '0.01s'}\n",
      "Final reward: -260.3295328745536, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   0%|          | 0/2400 [00:01<?, ?it/s, env_step=1600, len=200.00, n/ep=8.00, n/st=1600.00, rew=-219.15, rew_std=102.31, v/ep=26.30, v/st=5260.08]\n",
      "Epoch #1:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_result': '-198.14 ± 119.25',\n",
      " 'best_reward': -198.14143381569207,\n",
      " 'duration': '1.70s',\n",
      " 'test_episode': 100.0,\n",
      " 'test_speed': '14372.89 step/s',\n",
      " 'test_step': 20000,\n",
      " 'test_time': '1.39s',\n",
      " 'train_episode': 8.0,\n",
      " 'train_speed': '5167.00 step/s',\n",
      " 'train_step': 1600,\n",
      " 'train_time/collector': '0.30s',\n",
      " 'train_time/model': '0.01s'}\n",
      "Final reward: -127.19963327142413, length: 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 481it [00:03, 140.55it/s, env_step=8000, len=200, n/ep=8, n/st=1600, rew=-226.01, v/ep=26.78, v/st=5355.20]                         \n",
      "Epoch #2:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -340.829180 ± 227.785150, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 481it [00:03, 142.27it/s, env_step=17600, len=200, n/ep=8, n/st=1600, rew=-250.82, v/ep=26.75, v/st=5349.32]                         \n",
      "Epoch #3:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -421.689846 ± 276.842119, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 481it [00:03, 141.03it/s, env_step=27200, len=200, n/ep=8, n/st=1600, rew=-156.84, v/ep=26.68, v/st=5335.44]                         \n",
      "Epoch #4:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -520.911354 ± 255.808791, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 481it [00:03, 140.62it/s, env_step=36800, len=200, n/ep=8, n/st=1600, rew=-181.01, v/ep=26.49, v/st=5298.45]                         \n",
      "Epoch #5:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -376.586001 ± 261.626159, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 481it [00:03, 139.61it/s, env_step=46400, len=200, n/ep=8, n/st=1600, rew=-206.05, v/ep=26.56, v/st=5311.11]                         \n",
      "Epoch #6:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -462.374770 ± 329.470723, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 481it [00:03, 139.62it/s, env_step=56000, len=200, n/ep=8, n/st=1600, rew=-221.40, v/ep=26.35, v/st=5270.72]                         \n",
      "Epoch #7:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -378.144179 ± 222.875786, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 481it [00:03, 136.55it/s, env_step=65600, len=200, n/ep=8, n/st=1600, rew=-233.54, v/ep=25.43, v/st=5086.25]                         \n",
      "Epoch #8:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -393.466410 ± 215.599042, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 481it [00:03, 140.41it/s, env_step=75200, len=200, n/ep=8, n/st=1600, rew=-209.63, v/ep=26.49, v/st=5298.14]                         \n",
      "Epoch #9:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -469.960507 ± 197.303619, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 481it [00:03, 140.10it/s, env_step=84800, len=200, n/ep=8, n/st=1600, rew=-202.85, v/ep=26.55, v/st=5309.39]                         \n",
      "Epoch #10:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -381.124571 ± 232.391805, best_reward: -340.829180 ± 227.785150 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 481it [00:03, 125.23it/s, env_step=94400, len=200, n/ep=8, n/st=1600, rew=-301.90, v/ep=23.40, v/st=4680.19]                         \n",
      "Epoch #11:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -322.097723 ± 203.623002, best_reward: -322.097723 ± 203.623002 in #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 481it [00:04, 116.59it/s, env_step=104000, len=200, n/ep=8, n/st=1600, rew=-161.30, v/ep=22.50, v/st=4500.66]                         \n",
      "Epoch #12:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: -367.457856 ± 222.676505, best_reward: -322.097723 ± 203.623002 in #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12:  67%|######6   | 320/480 [00:02<00:01, 121.12it/s, env_step=110400, len=200, n/ep=8, n/st=1600, rew=-190.62, v/ep=23.25, v/st=4649.17]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c4002e47ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0mdistilled_policy_test_collector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_per_epoch\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_per_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                     args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/continuous/tianshou/test/continuous/distral/distral_offpolicy_trainer.py\u001b[0m in \u001b[0;36mDistral_offpolicy_trainer\u001b[0;34m(policy, train_collector1, train_collector2, test_collector, max_epoch, step_per_epoch, collect_per_step, episode_per_test, batch_size, update_per_step, train_fn, test_fn, stop_fn, save_fn, writer, log_interval, verbose, test_in_train, exist_distilled_policy, distilled_policy)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mresult1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_collector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollect_per_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0menv_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n/st\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 data1 = {\n",
      "\u001b[0;32m~/anaconda3/envs/continuous/lib/python3.6/site-packages/tianshou/data/collector.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad)\u001b[0m\n\u001b[1;32m    306\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_buf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_buf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/continuous/lib/python3.6/site-packages/tianshou/data/batch.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    239\u001b[0m                     \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                     \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/continuous/lib/python3.6/site-packages/tianshou/data/batch.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_items\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                     \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test pi_0\n",
    "import os\n",
    "import gym\n",
    "import torch\n",
    "import pprint\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.trainer import offpolicy_trainer    ##\n",
    "from tianshou.data import Collector, ReplayBuffer ##\n",
    "from tianshou.utils.net.continuous import Actor, ActorProb, Critic\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# The belows are special for policy distral\n",
    "from distral_offpolicy_trainer import Distral_offpolicy_trainer\n",
    "####from distral_collector import Distral_Collector    # 惊了，这个也不需要\n",
    "from distral_task_policy import TaskPolicy\n",
    "from distral_distilled_policy import DistilledPolicy\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--task', type=str, default='Swimmer-v2')  # ‘Pendulum-v0’\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--buffer-size', type=int, default=20000)\n",
    "    parser.add_argument('--actor-lr', type=float, default=3e-4)\n",
    "    parser.add_argument('--critic-lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--il-lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--tau', type=float, default=0.005)\n",
    "    parser.add_argument('--alpha', type=float, default=0.2)\n",
    "    parser.add_argument('--epoch', type=int, default=20)\n",
    "    parser.add_argument('--step-per-epoch', type=int, default=2400)\n",
    "    parser.add_argument('--collect-per-step', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--layer-num', type=int, default=1)\n",
    "    parser.add_argument('--training-num', type=int, default=8)\n",
    "    parser.add_argument('--test-num', type=int, default=100)\n",
    "    parser.add_argument('--logdir', type=str, default='log')\n",
    "    parser.add_argument('--render', type=float, default=0.)\n",
    "    parser.add_argument('--rew-norm', type=int, default=1)\n",
    "    parser.add_argument('--ignore-done', type=int, default=1)\n",
    "    parser.add_argument('--n-step', type=int, default=4)\n",
    "    parser.add_argument(\n",
    "        '--device', type=str,\n",
    "        default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    return args\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "#args.step_per_epoch = 800\n",
    "#args.epoch = 2\n",
    "\n",
    "torch.set_num_threads(1)  # we just need only one thread for NN\n",
    "env = gym.make(args.task)\n",
    "if args.task == 'Pendulum-v0':\n",
    "    env.spec.reward_threshold = -250\n",
    "args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "args.action_shape = env.action_space.shape or env.action_space.n\n",
    "args.max_action = env.action_space.high[0]\n",
    "# you can also use tianshou.env.SubprocVectorEnv\n",
    "# train_envs = gym.make(args.task)\n",
    "\n",
    "# model 1\n",
    "\n",
    "train_envs_1 = DummyVectorEnv(\n",
    "    [lambda: gym.make(args.task) for _ in range(args.training_num)])\n",
    "# test_envs = gym.make(args.task)\n",
    "test_envs_1 = DummyVectorEnv(\n",
    "    [lambda: gym.make(args.task) for _ in range(args.test_num)])\n",
    "\n",
    "# seed\n",
    "train_envs_1.seed(args.seed)\n",
    "test_envs_1.seed(args.seed)\n",
    "\n",
    "net_1 = Net(args.layer_num, args.state_shape, device=args.device)\n",
    "actor_1 = ActorProb(\n",
    "    net_1, args.action_shape, args.max_action, args.device, unbounded=True\n",
    ").to(args.device)\n",
    "actor_optim_1 = torch.optim.Adam(actor_1.parameters(), lr=args.actor_lr)\n",
    "net_c1_1 = Net(args.layer_num, args.state_shape,\n",
    "               args.action_shape, concat=True, device=args.device)\n",
    "critic1_1 = Critic(net_c1_1, args.device).to(args.device)\n",
    "critic1_optim_1 = torch.optim.Adam(critic1_1.parameters(), lr=args.critic_lr)\n",
    "net_c2_1 = Net(args.layer_num, args.state_shape,\n",
    "               args.action_shape, concat=True, device=args.device)\n",
    "critic2_1 = Critic(net_c2_1, args.device).to(args.device)\n",
    "critic2_optim_1 = torch.optim.Adam(critic2_1.parameters(), lr=args.critic_lr)\n",
    "policy_1 = TaskPolicy(\n",
    "    actor_1, actor_optim_1, critic1_1, critic1_optim_1, critic2_1, critic2_optim_1,\n",
    "    action_range=[env.action_space.low[0], env.action_space.high[0]],\n",
    "    tau=args.tau, gamma=args.gamma, alpha=args.alpha,\n",
    "    reward_normalization=args.rew_norm,\n",
    "    ignore_done=args.ignore_done,\n",
    "    estimation_step=args.n_step)\n",
    "\n",
    "# model 2\n",
    "\n",
    "train_envs_2 = DummyVectorEnv(\n",
    "    [lambda: gym.make(args.task) for _ in range(args.training_num)])\n",
    "# test_envs = gym.make(args.task)\n",
    "test_envs_2 = DummyVectorEnv(\n",
    "    [lambda: gym.make(args.task) for _ in range(args.test_num)])\n",
    "\n",
    "# seed\n",
    "train_envs_2.seed(args.seed)\n",
    "test_envs_2.seed(args.seed)\n",
    "\n",
    "net_2 = Net(args.layer_num, args.state_shape, device=args.device)\n",
    "actor_2 = ActorProb(\n",
    "    net_2, args.action_shape, args.max_action, args.device, unbounded=True\n",
    ").to(args.device)\n",
    "actor_optim_2 = torch.optim.Adam(actor_2.parameters(), lr=args.actor_lr)\n",
    "net_c1_2 = Net(args.layer_num, args.state_shape,\n",
    "               args.action_shape, concat=True, device=args.device)\n",
    "critic1_2 = Critic(net_c1_2, args.device).to(args.device)\n",
    "critic1_optim_2 = torch.optim.Adam(critic1_2.parameters(), lr=args.critic_lr)\n",
    "net_c2_2 = Net(args.layer_num, args.state_shape,\n",
    "               args.action_shape, concat=True, device=args.device)\n",
    "critic2_2 = Critic(net_c2_2, args.device).to(args.device)\n",
    "critic2_optim_2 = torch.optim.Adam(critic2_2.parameters(), lr=args.critic_lr)\n",
    "policy_2 = TaskPolicy(\n",
    "    actor_2, actor_optim_2, critic1_2, critic1_optim_2, critic2_2, critic2_optim_2,\n",
    "    action_range=[env.action_space.low[0], env.action_space.high[0]],\n",
    "    tau=args.tau, gamma=args.gamma, alpha=args.alpha,\n",
    "    reward_normalization=args.rew_norm,\n",
    "    ignore_done=args.ignore_done,\n",
    "    estimation_step=args.n_step)\n",
    "\n",
    "\n",
    "# collector\n",
    "train_collector_1 = Collector(\n",
    "    policy_1, train_envs_1, ReplayBuffer(args.buffer_size))\n",
    "test_collector_1 = Collector(policy_1, test_envs_1)\n",
    "train_collector_2 = Collector(\n",
    "    policy_2, train_envs_2, ReplayBuffer(args.buffer_size))\n",
    "test_collector_2 = Collector(policy_2, test_envs_2)\n",
    "# train_collector.collect(n_step=args.buffer_size)\n",
    "# log\n",
    "log_path = os.path.join(args.logdir, args.task, 'sac_distral')\n",
    "writer = SummaryWriter(log_path)\n",
    "\n",
    "\n",
    "def save_fn(policy):\n",
    "    torch.save(policy.state_dict(), os.path.join(log_path, 'policy.pth'))\n",
    "\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= env.spec.reward_threshold\n",
    "\n",
    "\n",
    "for itr in range(10):\n",
    "    if itr == 0:\n",
    "        # train policy 1\n",
    "        result = offpolicy_trainer(\n",
    "                policy_1, train_collector_1, test_collector_1, args.epoch,\n",
    "                args.step_per_epoch, args.collect_per_step, args.test_num,\n",
    "                args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n",
    "        if __name__ == '__main__':\n",
    "            pprint.pprint(result)\n",
    "            # Let's watch its performance!\n",
    "            policy_1.eval()\n",
    "            collector = Collector(policy_1, env)\n",
    "            result = collector.collect(n_episode=1, render=args.render)\n",
    "            print(f'Final reward: {result[\"rew\"]}, length: {result[\"len\"]}')\n",
    "        \n",
    "        # train policy 2\n",
    "        result = offpolicy_trainer(\n",
    "                policy_2, train_collector_2, test_collector_2, args.epoch,\n",
    "                args.step_per_epoch, args.collect_per_step, args.test_num,\n",
    "                args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n",
    "        if __name__ == '__main__':\n",
    "            pprint.pprint(result)\n",
    "            # Let's watch its performance!\n",
    "            policy_2.eval()\n",
    "            collector = Collector(policy_2, env)\n",
    "            result = collector.collect(n_episode=1, render=args.render)\n",
    "            print(f'Final reward: {result[\"rew\"]}, length: {result[\"len\"]}')\n",
    "        \n",
    "        # train distilled policy\n",
    "        if args.task == 'Pendulum-v0':\n",
    "            env.spec.reward_threshold = -300  # lower the goal\n",
    "        net = ActorProb(\n",
    "                Net(1, args.state_shape, device=args.device),\n",
    "                args.action_shape, args.max_action, args.device\n",
    "                ).to(args.device)\n",
    "        \n",
    "        optim = torch.optim.Adam(net.parameters(), lr=args.il_lr)\n",
    "\n",
    "        distilled_policy = DistilledPolicy(net, optim, \n",
    "                    action_range=[env.action_space.low[0], env.action_space.high[0]],\n",
    "                            mode='continuous')\n",
    "\n",
    "        distilled_policy_test_collector = Collector(\n",
    "                    distilled_policy,\n",
    "                    DummyVectorEnv(\n",
    "                        [lambda: gym.make(args.task) for _ in range(args.test_num)])\n",
    "                )\n",
    "\n",
    "        train_collector_1.reset()\n",
    "        train_collector_2.reset()\n",
    "\n",
    "        result = Distral_offpolicy_trainer(\n",
    "                    distilled_policy, train_collector_1, train_collector_2, \n",
    "                    distilled_policy_test_collector, 20,\n",
    "                    args.step_per_epoch // 5, args.collect_per_step, args.test_num,\n",
    "                    args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n",
    "        if __name__ == '__main__':\n",
    "            pprint.pprint(result)\n",
    "            # Let's watch its performance!\n",
    "            distilled_policy.eval()\n",
    "            collector = Collector(distilled_policy, env)\n",
    "            result = collector.collect(n_episode=1, render=args.render)\n",
    "            print(f'Final reward: {result[\"rew\"]}, length: {result[\"len\"]}')\n",
    "    else:\n",
    "        # train policy 1\n",
    "        result = offpolicy_trainer(\n",
    "                policy_1, train_collector_1, test_collector_1, args.epoch,\n",
    "                args.step_per_epoch, args.collect_per_step, args.test_num,\n",
    "                args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n",
    "        if __name__ == '__main__':\n",
    "            pprint.pprint(result)\n",
    "            # Let's watch its performance!\n",
    "            policy_1.eval()\n",
    "            collector = Collector(policy_1, env)\n",
    "            result = collector.collect(n_episode=1, render=args.render)\n",
    "            print(f'Final reward: {result[\"rew\"]}, length: {result[\"len\"]}')\n",
    "        # train policy 2\n",
    "        result = offpolicy_trainer(\n",
    "                policy_2, train_collector_2, test_collector_2, args.epoch,\n",
    "                args.step_per_epoch, args.collect_per_step, args.test_num,\n",
    "                args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n",
    "        if __name__ == '__main__':\n",
    "            pprint.pprint(result)\n",
    "            # Let's watch its performance!\n",
    "            policy_2.eval()\n",
    "            collector = Collector(policy_2, env)\n",
    "            result = collector.collect(n_episode=1, render=args.render)\n",
    "            print(f'Final reward: {result[\"rew\"]}, length: {result[\"len\"]}')\n",
    "        # train distilled policy\n",
    "        train_collector_1.reset()\n",
    "        train_collector_2.reset()\n",
    "        \n",
    "        result = Distral_offpolicy_trainer(\n",
    "                    distilled_policy, train_collector_1, train_collector_2, \n",
    "                    distilled_policy_test_collector, 20,\n",
    "                    args.step_per_epoch // 5, args.collect_per_step, args.test_num,\n",
    "                    args.batch_size, stop_fn=stop_fn, save_fn=save_fn, writer=writer)\n",
    "        if __name__ == '__main__':\n",
    "            pprint.pprint(result)\n",
    "            # Let's watch its performance!\n",
    "            distilled_policy.eval()\n",
    "            collector = Collector(distilled_policy, env)\n",
    "            result = collector.collect(n_episode=1, render=args.render)\n",
    "            print(f'Final reward: {result[\"rew\"]}, length: {result[\"len\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
